{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet: Classification in Python\n",
    "### Author: Antons Ruberts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebok gives a complete example of how to use TabNet for binary classification example. You can download the data from [here](https://www.kaggle.com/c/ieee-fraud-detection) and follow this example along. Alternatively, you can substitute it with your own data but you'll need to make appropriate changes to the pre-processing code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from tensorflow_addons.activations import sparsemax\n",
    "from scipy.special import softmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into 2 files, so we need to join them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.96501\n",
       "1    0.03499\n",
       "Name: isFraud, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transactions = pd.read_csv('../train_transaction.csv')\n",
    "train_identity = pd.read_csv('../train_identity.csv')\n",
    "\n",
    "# merge two datasets\n",
    "train = pd.merge(train_transactions, train_identity, on='TransactionID', how='left')\n",
    "train['isFraud'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transactions = pd.read_csv('../test_transaction.csv')\n",
    "test_identity = pd.read_csv('../test_identity.csv')\n",
    "\n",
    "# merge two datasets\n",
    "test = pd.merge(test_transactions, test_identity, on='TransactionID', how='left')\n",
    "\n",
    "test.columns = [c.replace('-', '_') for c in test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is the most crucial step in the fraud detection domain. However, since it's not the main goal of this project, I'm skipping this step and leaving it to the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an hour feature from datetime stamp \n",
    "def make_hour_feature(f):\n",
    "    #Creates an hour of the day feature, encoded as 0-23.  \n",
    "    hours = f / (3600)        \n",
    "    encoded_hours = np.floor(hours) % 24\n",
    "    return encoded_hours\n",
    "\n",
    "train['hour'] = make_hour_feature(train['TransactionDT'])\n",
    "test['hour'] = make_hour_feature(test['TransactionDT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Here's the cleaning that we need to do:\n",
    "* Drop columns with too many missing values\n",
    "* Impute numeric missing values with median\n",
    "* Impute categorical missing values with \"missing\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', \n",
    "               'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1',\n",
    "               'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'DeviceInfo',\n",
    "               'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20',\n",
    "               'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
    "               'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\n",
    "\n",
    "exclude = ['TransactionID', 'TransactionDT', 'isFraud']\n",
    "num_features = [f for f in train.columns if (f not in cat_features) & (f not in exclude)]\n",
    "\n",
    "# drop more than 90% NAs\n",
    "col_na = train.isna().sum()\n",
    "to_drop = col_na[(col_na / train.shape[0]) > 0.9].index\n",
    "\n",
    "use_cols = [f for f in train.columns if f not in to_drop]\n",
    "cat_features = [f for f in cat_features if f not in to_drop]\n",
    "num_features = [f for f in num_features if f not in to_drop]\n",
    "\n",
    "train[cat_features] = train[cat_features].astype(str)\n",
    "train[num_features] = train[num_features].astype(np.float)\n",
    "train = train[use_cols]\n",
    "\n",
    "test[cat_features] = test[cat_features].astype(str)\n",
    "test[num_features] = test[num_features].astype(np.float)\n",
    "test = test[[f for f in use_cols if f != 'isFraud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill numeric NAs with median\n",
    "median_values = train[num_features].median() \n",
    "train[num_features] = train[num_features].fillna(median_values)\n",
    "\n",
    "# fill categorical NAs with \"missing\"\n",
    "train[cat_features] = train[cat_features].replace(\"nan\", \"missing\")\n",
    "\n",
    "train.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[num_features] = test[num_features].fillna(median_values)\n",
    "\n",
    "# fill categorical NAs with \"missing\"\n",
    "test[cat_features] = test[cat_features].replace(\"nan\", \"missing\")\n",
    "test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 423)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation split is done using datetime column since testing will be performed on the future time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split based on time\n",
    "train_split = train['TransactionDT'] <= np.quantile(train['TransactionDT'], 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.loc[train_split.values, num_features+cat_features]\n",
    "train_y = train.loc[train_split.values,'isFraud']\n",
    "\n",
    "val_X = train.loc[~train_split.values, num_features+cat_features]\n",
    "val_y = train.loc[~train_split.values,'isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531486 train examples\n",
      "59054 validation examples\n"
     ]
    }
   ],
   "source": [
    "print(len(train_X), 'train examples')\n",
    "print(len(val_X), 'validation examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since neural network can process only numeric data, the input should be preprocessed. This is a very simplified pre-processing pipeline, and can be replaced with something more sophisticated like category embeddings. \n",
    "\n",
    "* Scale the numeric features\n",
    "* Encode the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns will be scaled by StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Categorical will be transformed using Weight of Evidence approach\n",
    "woe = WOEEncoder()\n",
    "\n",
    "column_trans = ColumnTransformer(\n",
    "    [ ('scaler',scaler, num_features),\n",
    "    ('woe', woe, cat_features)], remainder='passthrough', n_jobs=-1)\n",
    "\n",
    "train_X_transformed = column_trans.fit_transform(train_X, train_y)\n",
    "val_X_transformed = column_trans.transform(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531486, 420) (59054, 420) (506691, 420)\n"
     ]
    }
   ],
   "source": [
    "test_X_transformed = column_trans.transform(test[num_features + cat_features])\n",
    "\n",
    "print(train_X_transformed.shape, val_X_transformed.shape, test_X_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_transformed = pd.DataFrame(train_X_transformed, columns=[num_features + cat_features])\n",
    "val_X_transformed = pd.DataFrame(val_X_transformed, columns=[num_features + cat_features])\n",
    "test_X_trinsformed = pd.DataFrame(test_X_transformed, columns=[num_features + cat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training and inference faster, we need to transform the data into TF Data object. You can find out more about it at the [TF website](https://www.tensorflow.org/guide/data).\n",
    "\n",
    "*Large batch sizes are used per paper instructions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tf_dataset(\n",
    "    X,\n",
    "    batch_size,\n",
    "    y = None,\n",
    "    shuffle = False,\n",
    "    drop_remainder = False,\n",
    "):\n",
    "    size_of_dataset = len(X)\n",
    "    if y is not None:\n",
    "        y = tf.one_hot(y.astype(int), 2)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((np.array(X.astype(np.float32)), y))\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(np.array(X.astype(np.float32)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=size_of_dataset)\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "    autotune = tf.data.experimental.AUTOTUNE\n",
    "    ds = ds.prefetch(autotune)\n",
    "    return ds\n",
    "\n",
    "train_ds = prepare_tf_dataset(train_X_transformed, 16384, train_y)\n",
    "val_ds = prepare_tf_dataset(val_X_transformed, 16384, val_y)\n",
    "test_ds = prepare_tf_dataset(test_X_transformed, 16384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines all the classes/functions covered in the blog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glu(x, n_units=None):\n",
    "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
    "    return x[:, :n_units] * tf.nn.sigmoid(x[:, n_units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBlock(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Implementation of a FL->BN->GLU block\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        apply_glu = True,\n",
    "        bn_momentum = 0.9,\n",
    "        fc = None,\n",
    "        epsilon = 1e-5,\n",
    "    ):\n",
    "        super(FeatureBlock, self).__init__()\n",
    "        self.apply_gpu = apply_glu\n",
    "        self.feature_dim = feature_dim\n",
    "        units = feature_dim * 2 if apply_glu else feature_dim # desired dimension gets multiplied by 2\n",
    "                                                              # because GLU activation halves it\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(units, use_bias=False) if fc is None else fc # shared layers can get re-used\n",
    "        self.bn = tf.keras.layers.BatchNormalization(momentum=bn_momentum, epsilon=epsilon)\n",
    "\n",
    "    def call(self, x, training = None):\n",
    "        x = self.fc(x) # inputs passes through the FC layer\n",
    "        x = self.bn(x, training=training) # FC layer output gets passed through the BN\n",
    "        if self.apply_gpu: \n",
    "            return glu(x, self.feature_dim) # GLU activation applied to BN output\n",
    "        return x\n",
    "\n",
    "    \n",
    "class FeatureTransformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        fcs = [],\n",
    "        n_total = 4,\n",
    "        n_shared = 2,\n",
    "        bn_momentum = 0.9,\n",
    "    ):\n",
    "        super(FeatureTransformer, self).__init__()\n",
    "        self.n_total, self.n_shared = n_total, n_shared\n",
    "\n",
    "        kwrgs = {\n",
    "            \"feature_dim\": feature_dim,\n",
    "            \"bn_momentum\": bn_momentum,\n",
    "        }\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = []\n",
    "        for n in range(n_total):\n",
    "            # some shared blocks\n",
    "            if fcs and n < len(fcs):\n",
    "                self.blocks.append(FeatureBlock(**kwrgs, fc=fcs[n])) # Building shared blocks by providing FC layers\n",
    "            # build new blocks\n",
    "            else:\n",
    "                self.blocks.append(FeatureBlock(**kwrgs)) # Step dependent blocks without the shared FC layers\n",
    "\n",
    "    def call(self, x, training = None):\n",
    "        # input passes through the first block\n",
    "        x = self.blocks[0](x, training=training) \n",
    "        # for the remaining blocks\n",
    "        for n in range(1, self.n_total):\n",
    "            # output from previous block gets multiplied by sqrt(0.5) and output of this block gets added\n",
    "            x = x * tf.sqrt(0.5) + self.blocks[n](x, training=training) \n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def shared_fcs(self):\n",
    "        return [self.blocks[i].fc for i in range(self.n_shared)]\n",
    "    \n",
    "class AttentiveTransformer(tf.keras.Model):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.block = FeatureBlock(\n",
    "            feature_dim,\n",
    "            apply_glu=False,\n",
    "        )\n",
    "\n",
    "    def call(self, x, prior_scales, training=None):\n",
    "        x = self.block(x, training=training)\n",
    "        return sparsemax(x * prior_scales)\n",
    "    \n",
    "class TabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        feature_dim,\n",
    "        output_dim,\n",
    "        n_step = 2,\n",
    "        n_total = 4,\n",
    "        n_shared = 2,\n",
    "        relaxation_factor = 1.5,\n",
    "        bn_epsilon = 1e-5,\n",
    "        bn_momentum = 0.7,\n",
    "        sparsity_coefficient = 1e-5\n",
    "    ):\n",
    "        super(TabNet, self).__init__()\n",
    "        self.output_dim, self.num_features = output_dim, num_features\n",
    "        self.n_step, self.relaxation_factor = n_step, relaxation_factor\n",
    "        self.sparsity_coefficient = sparsity_coefficient\n",
    "\n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "            momentum=bn_momentum, epsilon=bn_epsilon\n",
    "        )\n",
    "\n",
    "        kargs = {\n",
    "            \"feature_dim\": feature_dim + output_dim,\n",
    "            \"n_total\": n_total,\n",
    "            \"n_shared\": n_shared,\n",
    "            \"bn_momentum\": bn_momentum\n",
    "        }\n",
    "\n",
    "        # first feature transformer block is built first to get the shared blocks\n",
    "        self.feature_transforms = [FeatureTransformer(**kargs)]\n",
    "        self.attentive_transforms = []\n",
    "            \n",
    "        # each step consists out of FT and AT\n",
    "        for i in range(n_step):\n",
    "            self.feature_transforms.append(\n",
    "                FeatureTransformer(**kargs, fcs=self.feature_transforms[0].shared_fcs)\n",
    "            )\n",
    "            self.attentive_transforms.append(\n",
    "                AttentiveTransformer(num_features)\n",
    "            )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.head = tf.keras.layers.Dense(2, activation=\"softmax\", use_bias=False)\n",
    "\n",
    "    def call(self, features, training = None):\n",
    "\n",
    "        bs = tf.shape(features)[0] # get batch shape\n",
    "        out_agg = tf.zeros((bs, self.output_dim)) # empty array with outputs to fill\n",
    "        prior_scales = tf.ones((bs, self.num_features)) # prior scales initialised as 1s\n",
    "        importance = tf.zeros([bs, self.num_features]) # importances\n",
    "        masks = []\n",
    "\n",
    "        features = self.bn(features, training=training) # Batch Normalisation\n",
    "        masked_features = features\n",
    "\n",
    "        total_entropy = 0.0\n",
    "\n",
    "        for step_i in range(self.n_step + 1):\n",
    "            # (masked) features go through the FT\n",
    "            x = self.feature_transforms[step_i](\n",
    "                masked_features, training=training\n",
    "            )\n",
    "            \n",
    "            # first FT is not used to generate output\n",
    "            if step_i > 0:\n",
    "                # first half of the FT output goes towards the decision \n",
    "                out = tf.keras.activations.relu(x[:, : self.output_dim])\n",
    "                out_agg += out\n",
    "                scale_agg = tf.reduce_sum(out, axis=1, keepdims=True) / (self.n_step - 1)\n",
    "                importance += mask_values * scale_agg\n",
    "                \n",
    "\n",
    "            # no need to build the features mask for the last step\n",
    "            if step_i < self.n_step:\n",
    "                # second half of the FT output goes as input to the AT\n",
    "                x_for_mask = x[:, self.output_dim :]\n",
    "                \n",
    "                # apply AT with prior scales\n",
    "                mask_values = self.attentive_transforms[step_i](\n",
    "                    x_for_mask, prior_scales, training=training\n",
    "                )\n",
    "\n",
    "                # recalculate the prior scales\n",
    "                prior_scales *= self.relaxation_factor - mask_values\n",
    "                \n",
    "                # multiply the second half of the FT output by the attention mask to enforce sparsity\n",
    "                masked_features = tf.multiply(mask_values, features)\n",
    "\n",
    "                # entropy is used to penalize the amount of sparsity in feature selection\n",
    "                total_entropy += tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.multiply(-mask_values, tf.math.log(mask_values + 1e-15)),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # append mask values for later explainability\n",
    "                masks.append(tf.expand_dims(tf.expand_dims(mask_values, 0), 3))\n",
    "                \n",
    "        #Per step selection masks        \n",
    "        self.selection_masks = masks\n",
    "        \n",
    "        # Final output\n",
    "        final_output = self.head(out)\n",
    "        \n",
    "        # Add sparsity loss\n",
    "        loss = total_entropy / (self.n_step-1)\n",
    "        self.add_loss(self.sparsity_coefficient * loss)\n",
    "        \n",
    "        return final_output, importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Tuning\n",
    "#### Don't run if short on resources/time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabNet has quite a few parameters to tune. Below you can find the ranges which I usually use in tuning. There's no one-size-fits-all parameter set, so make sure to adjust uit for your problem.\n",
    "\n",
    "* Feature Dimension - between 32 and 512\n",
    "* Number of steps - from 2 to 9\n",
    "* Relaxation factor - from 1 to 3\n",
    "* Sparsity Coefficiet - from 0 to 0.1\n",
    "\n",
    "Optional parameters to tune:\n",
    "* Batch Momentum - from 0.9 to 0.9999\n",
    "* Class weight - from 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "\n",
    "def Objective(trial):\n",
    "    feature_dim = trial.suggest_categorical(\"feature_dim\", [32, 64, 128, 256, 512])\n",
    "    n_step = trial.suggest_int(\"n_step\", 2, 9, step=1)\n",
    "    n_shared = trial.suggest_int(\"n_shared\", 0, 4, step=1)\n",
    "    relaxation_factor = trial.suggest_float(\"relaxation_factor\", 1., 3., step=0.1)\n",
    "    sparsity_coefficient = trial.suggest_float(\"sparsity_coefficient\", 0.00000001, 0.1, log=True)\n",
    "    bn_momentum = trial.suggest_float('bn_momentum', 0.9, 0.9999)\n",
    "    tabnet_params = dict(num_features=train_X_transformed.shape[1],\n",
    "                         output_dim=feature_dim,\n",
    "                         feature_dim=feature_dim,\n",
    "                         n_step=n_step, \n",
    "                         relaxation_factor=relaxation_factor,\n",
    "                         sparsity_coefficient=sparsity_coefficient,\n",
    "                         n_shared = n_shared,\n",
    "                         bn_momentum = bn_momentum\n",
    "                     )\n",
    "    class_weight =  trial.suggest_int(\"class_weight\", 1, 10.5, step=0.5)\n",
    "    \n",
    "    cbs = [tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "        )]\n",
    "    \n",
    "    tn = TabNet(**tabnet_params)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,clipnorm=10)\n",
    "    loss = [tf.keras.losses.CategoricalCrossentropy(from_logits=False),None]\n",
    "    \n",
    "    tn.compile(\n",
    "            optimizer,\n",
    "            loss=loss)\n",
    "\n",
    "    tn.fit(train_ds, \n",
    "          epochs=100, \n",
    "          validation_data=val_ds,\n",
    "          callbacks=cbs,\n",
    "          verbose=1)\n",
    "    \n",
    "    \n",
    "    val_preds, _ =  tn.predict(val_ds)\n",
    "    pr_auc = average_precision_score(val_y, val_preds[:,1])\n",
    "    \n",
    "    return pr_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name='TabNet optimization')\n",
    "study.optimize(Objective, n_jobs=1, n_trials=100, gc_after_trial=True, show_progress_bar=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "33/33 [==============================] - 13s 197ms/step - loss: 0.7555 - output_1_loss: 0.7555 - val_loss: 0.4413 - val_output_1_loss: 0.4413\n",
      "Epoch 2/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.6034 - output_1_loss: 0.6034 - val_loss: 0.2936 - val_output_1_loss: 0.2936\n",
      "Epoch 3/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.5595 - output_1_loss: 0.5595 - val_loss: 0.2791 - val_output_1_loss: 0.2791\n",
      "Epoch 4/1000\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.5347 - output_1_loss: 0.5347 - val_loss: 0.2522 - val_output_1_loss: 0.2522\n",
      "Epoch 5/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.5001 - output_1_loss: 0.5001 - val_loss: 0.2545 - val_output_1_loss: 0.2545\n",
      "Epoch 6/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.4944 - output_1_loss: 0.4944 - val_loss: 0.2756 - val_output_1_loss: 0.2756\n",
      "Epoch 7/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.4737 - output_1_loss: 0.4737 - val_loss: 0.2764 - val_output_1_loss: 0.2764\n",
      "Epoch 8/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.4600 - output_1_loss: 0.4600 - val_loss: 0.4934 - val_output_1_loss: 0.4934\n",
      "Epoch 9/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.4553 - output_1_loss: 0.4553 - val_loss: 0.6160 - val_output_1_loss: 0.6160\n",
      "Epoch 10/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.4456 - output_1_loss: 0.4456 - val_loss: 0.2383 - val_output_1_loss: 0.2383\n",
      "Epoch 11/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.4303 - output_1_loss: 0.4303 - val_loss: 0.4255 - val_output_1_loss: 0.4255\n",
      "Epoch 12/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.4221 - output_1_loss: 0.4221 - val_loss: 0.3135 - val_output_1_loss: 0.3135\n",
      "Epoch 13/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.4108 - output_1_loss: 0.4108 - val_loss: 0.2211 - val_output_1_loss: 0.2211\n",
      "Epoch 14/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.4074 - output_1_loss: 0.4074 - val_loss: 0.2149 - val_output_1_loss: 0.2149\n",
      "Epoch 15/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3990 - output_1_loss: 0.3990 - val_loss: 0.2013 - val_output_1_loss: 0.2013\n",
      "Epoch 16/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3940 - output_1_loss: 0.3940 - val_loss: 0.2134 - val_output_1_loss: 0.2134\n",
      "Epoch 17/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3864 - output_1_loss: 0.3864 - val_loss: 0.2061 - val_output_1_loss: 0.2061\n",
      "Epoch 18/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3830 - output_1_loss: 0.3830 - val_loss: 0.2283 - val_output_1_loss: 0.2283\n",
      "Epoch 19/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3850 - output_1_loss: 0.3850 - val_loss: 0.2606 - val_output_1_loss: 0.2606\n",
      "Epoch 20/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3958 - output_1_loss: 0.3958 - val_loss: 0.2175 - val_output_1_loss: 0.2175\n",
      "Epoch 21/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3850 - output_1_loss: 0.3850 - val_loss: 0.2472 - val_output_1_loss: 0.2472\n",
      "Epoch 22/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3827 - output_1_loss: 0.3827 - val_loss: 0.3356 - val_output_1_loss: 0.3356\n",
      "Epoch 23/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3816 - output_1_loss: 0.3816 - val_loss: 0.2331 - val_output_1_loss: 0.2331\n",
      "Epoch 24/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3710 - output_1_loss: 0.3710 - val_loss: 0.2447 - val_output_1_loss: 0.2447\n",
      "Epoch 25/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3617 - output_1_loss: 0.3617 - val_loss: 0.2140 - val_output_1_loss: 0.2140\n",
      "Epoch 26/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3594 - output_1_loss: 0.3594 - val_loss: 0.3242 - val_output_1_loss: 0.3242\n",
      "Epoch 27/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3610 - output_1_loss: 0.3610 - val_loss: 0.2015 - val_output_1_loss: 0.2015\n",
      "Epoch 28/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3570 - output_1_loss: 0.3570 - val_loss: 0.2179 - val_output_1_loss: 0.2179\n",
      "Epoch 29/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3504 - output_1_loss: 0.3504 - val_loss: 0.2328 - val_output_1_loss: 0.2328\n",
      "Epoch 30/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3446 - output_1_loss: 0.3446 - val_loss: 0.2738 - val_output_1_loss: 0.2738\n",
      "Epoch 31/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3431 - output_1_loss: 0.3431 - val_loss: 0.2129 - val_output_1_loss: 0.2129\n",
      "Epoch 32/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3374 - output_1_loss: 0.3374 - val_loss: 0.2526 - val_output_1_loss: 0.2526\n",
      "Epoch 33/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3418 - output_1_loss: 0.3418 - val_loss: 0.1963 - val_output_1_loss: 0.1963\n",
      "Epoch 34/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3357 - output_1_loss: 0.3357 - val_loss: 0.2245 - val_output_1_loss: 0.2245\n",
      "Epoch 35/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3295 - output_1_loss: 0.3295 - val_loss: 0.2329 - val_output_1_loss: 0.2329\n",
      "Epoch 36/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3198 - output_1_loss: 0.3198 - val_loss: 0.2224 - val_output_1_loss: 0.2224\n",
      "Epoch 37/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3113 - output_1_loss: 0.3113 - val_loss: 0.2096 - val_output_1_loss: 0.2096\n",
      "Epoch 38/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3037 - output_1_loss: 0.3037 - val_loss: 0.1917 - val_output_1_loss: 0.1917\n",
      "Epoch 39/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3169 - output_1_loss: 0.3169 - val_loss: 0.1996 - val_output_1_loss: 0.1996\n",
      "Epoch 40/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3294 - output_1_loss: 0.3294 - val_loss: 0.2064 - val_output_1_loss: 0.2064\n",
      "Epoch 41/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3111 - output_1_loss: 0.3111 - val_loss: 0.1994 - val_output_1_loss: 0.1994\n",
      "Epoch 42/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3050 - output_1_loss: 0.3050 - val_loss: 0.2245 - val_output_1_loss: 0.2245\n",
      "Epoch 43/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3006 - output_1_loss: 0.3006 - val_loss: 0.2169 - val_output_1_loss: 0.2169\n",
      "Epoch 44/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.3151 - output_1_loss: 0.3151 - val_loss: 0.2051 - val_output_1_loss: 0.2051\n",
      "Epoch 45/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.3036 - output_1_loss: 0.3036 - val_loss: 0.2068 - val_output_1_loss: 0.2068\n",
      "Epoch 46/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2993 - output_1_loss: 0.2993 - val_loss: 0.2206 - val_output_1_loss: 0.2206\n",
      "Epoch 47/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2953 - output_1_loss: 0.2953 - val_loss: 0.2101 - val_output_1_loss: 0.2101\n",
      "Epoch 48/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2920 - output_1_loss: 0.2920 - val_loss: 0.2246 - val_output_1_loss: 0.2246\n",
      "Epoch 49/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2804 - output_1_loss: 0.2804 - val_loss: 0.2126 - val_output_1_loss: 0.2126\n",
      "Epoch 50/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2745 - output_1_loss: 0.2745 - val_loss: 0.1963 - val_output_1_loss: 0.1963\n",
      "Epoch 51/1000\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.2746 - output_1_loss: 0.2746 - val_loss: 0.1882 - val_output_1_loss: 0.1882\n",
      "Epoch 52/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2804 - output_1_loss: 0.2804 - val_loss: 0.1901 - val_output_1_loss: 0.1901\n",
      "Epoch 53/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2828 - output_1_loss: 0.2828 - val_loss: 0.1886 - val_output_1_loss: 0.1886\n",
      "Epoch 54/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2631 - output_1_loss: 0.2631 - val_loss: 0.2008 - val_output_1_loss: 0.2008\n",
      "Epoch 55/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2684 - output_1_loss: 0.2684 - val_loss: 0.1905 - val_output_1_loss: 0.1905\n",
      "Epoch 56/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2724 - output_1_loss: 0.2724 - val_loss: 0.1981 - val_output_1_loss: 0.1981\n",
      "Epoch 57/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2729 - output_1_loss: 0.2729 - val_loss: 0.1790 - val_output_1_loss: 0.1790\n",
      "Epoch 58/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2627 - output_1_loss: 0.2627 - val_loss: 0.1897 - val_output_1_loss: 0.1897\n",
      "Epoch 59/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2577 - output_1_loss: 0.2577 - val_loss: 0.1858 - val_output_1_loss: 0.1858\n",
      "Epoch 60/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2621 - output_1_loss: 0.2621 - val_loss: 0.2092 - val_output_1_loss: 0.2092\n",
      "Epoch 61/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2732 - output_1_loss: 0.2732 - val_loss: 0.1998 - val_output_1_loss: 0.1998\n",
      "Epoch 62/1000\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.2630 - output_1_loss: 0.2630 - val_loss: 0.2255 - val_output_1_loss: 0.2255\n",
      "Epoch 63/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2561 - output_1_loss: 0.2561 - val_loss: 0.1981 - val_output_1_loss: 0.1981\n",
      "Epoch 64/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2436 - output_1_loss: 0.2436 - val_loss: 0.1868 - val_output_1_loss: 0.1868\n",
      "Epoch 65/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2422 - output_1_loss: 0.2422 - val_loss: 0.1952 - val_output_1_loss: 0.1952\n",
      "Epoch 66/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2515 - output_1_loss: 0.2515 - val_loss: 0.2383 - val_output_1_loss: 0.2383\n",
      "Epoch 67/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2610 - output_1_loss: 0.2610 - val_loss: 0.3148 - val_output_1_loss: 0.3148\n",
      "Epoch 68/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2538 - output_1_loss: 0.2538 - val_loss: 0.3588 - val_output_1_loss: 0.3588\n",
      "Epoch 69/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2518 - output_1_loss: 0.2518 - val_loss: 0.4546 - val_output_1_loss: 0.4546\n",
      "Epoch 70/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2636 - output_1_loss: 0.2636 - val_loss: 0.3448 - val_output_1_loss: 0.3448\n",
      "Epoch 71/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2477 - output_1_loss: 0.2477 - val_loss: 1.0349 - val_output_1_loss: 1.0349\n",
      "Epoch 72/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2433 - output_1_loss: 0.2433 - val_loss: 0.3557 - val_output_1_loss: 0.3557\n",
      "Epoch 73/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2453 - output_1_loss: 0.2453 - val_loss: 0.1824 - val_output_1_loss: 0.1824\n",
      "Epoch 74/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2432 - output_1_loss: 0.2432 - val_loss: 0.2454 - val_output_1_loss: 0.2454\n",
      "Epoch 75/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2450 - output_1_loss: 0.2450 - val_loss: 0.2764 - val_output_1_loss: 0.2764\n",
      "Epoch 76/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2434 - output_1_loss: 0.2434 - val_loss: 0.2820 - val_output_1_loss: 0.2820\n",
      "Epoch 77/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2508 - output_1_loss: 0.2508 - val_loss: 0.2275 - val_output_1_loss: 0.2275\n",
      "Epoch 78/1000\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.2656 - output_1_loss: 0.2656 - val_loss: 0.2180 - val_output_1_loss: 0.2180\n",
      "Epoch 79/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2414 - output_1_loss: 0.2414 - val_loss: 0.1782 - val_output_1_loss: 0.1782\n",
      "Epoch 80/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2327 - output_1_loss: 0.2327 - val_loss: 0.1779 - val_output_1_loss: 0.1779\n",
      "Epoch 81/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2289 - output_1_loss: 0.2289 - val_loss: 0.1897 - val_output_1_loss: 0.1897\n",
      "Epoch 82/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2243 - output_1_loss: 0.2243 - val_loss: 0.1807 - val_output_1_loss: 0.1807\n",
      "Epoch 83/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2234 - output_1_loss: 0.2234 - val_loss: 0.1829 - val_output_1_loss: 0.1829\n",
      "Epoch 84/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2293 - output_1_loss: 0.2293 - val_loss: 0.1802 - val_output_1_loss: 0.1802\n",
      "Epoch 85/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2486 - output_1_loss: 0.2486 - val_loss: 0.1886 - val_output_1_loss: 0.1886\n",
      "Epoch 86/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2363 - output_1_loss: 0.2363 - val_loss: 0.1817 - val_output_1_loss: 0.1817\n",
      "Epoch 87/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2221 - output_1_loss: 0.2221 - val_loss: 0.1881 - val_output_1_loss: 0.1881\n",
      "Epoch 88/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2157 - output_1_loss: 0.2157 - val_loss: 0.1800 - val_output_1_loss: 0.1800\n",
      "Epoch 89/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2078 - output_1_loss: 0.2078 - val_loss: 0.1894 - val_output_1_loss: 0.1894\n",
      "Epoch 90/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2115 - output_1_loss: 0.2115 - val_loss: 0.1926 - val_output_1_loss: 0.1926\n",
      "Epoch 91/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2101 - output_1_loss: 0.2101 - val_loss: 0.1889 - val_output_1_loss: 0.1889\n",
      "Epoch 92/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2037 - output_1_loss: 0.2037 - val_loss: 0.2012 - val_output_1_loss: 0.2012\n",
      "Epoch 93/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2009 - output_1_loss: 0.2009 - val_loss: 0.2160 - val_output_1_loss: 0.2160\n",
      "Epoch 94/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2087 - output_1_loss: 0.2087 - val_loss: 0.1886 - val_output_1_loss: 0.1886\n",
      "Epoch 95/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2167 - output_1_loss: 0.2167 - val_loss: 0.2045 - val_output_1_loss: 0.2045\n",
      "Epoch 96/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2063 - output_1_loss: 0.2063 - val_loss: 0.1978 - val_output_1_loss: 0.1978\n",
      "Epoch 97/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.1964 - output_1_loss: 0.1964 - val_loss: 0.2408 - val_output_1_loss: 0.2408\n",
      "Epoch 98/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.1932 - output_1_loss: 0.1932 - val_loss: 0.2698 - val_output_1_loss: 0.2698\n",
      "Epoch 99/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.1969 - output_1_loss: 0.1969 - val_loss: 0.1797 - val_output_1_loss: 0.1797\n",
      "Epoch 100/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2041 - output_1_loss: 0.2041 - val_loss: 0.3118 - val_output_1_loss: 0.3118\n",
      "Epoch 101/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.1910 - output_1_loss: 0.1910 - val_loss: 0.2006 - val_output_1_loss: 0.2006\n",
      "Epoch 102/1000\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.1924 - output_1_loss: 0.1924 - val_loss: 0.2844 - val_output_1_loss: 0.2844\n",
      "Epoch 103/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.1917 - output_1_loss: 0.1917 - val_loss: 0.2317 - val_output_1_loss: 0.2317\n",
      "Epoch 104/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.1993 - output_1_loss: 0.1993 - val_loss: 0.2257 - val_output_1_loss: 0.2256\n",
      "Epoch 105/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2056 - output_1_loss: 0.2056 - val_loss: 0.4014 - val_output_1_loss: 0.4013\n",
      "Epoch 106/1000\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.2046 - output_1_loss: 0.2046 - val_loss: 0.2066 - val_output_1_loss: 0.2066\n",
      "Epoch 107/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.1913 - output_1_loss: 0.1913 - val_loss: 0.2970 - val_output_1_loss: 0.2970\n",
      "Epoch 108/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.1949 - output_1_loss: 0.1949 - val_loss: 0.7228 - val_output_1_loss: 0.7228\n",
      "Epoch 109/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2049 - output_1_loss: 0.2049 - val_loss: 0.4805 - val_output_1_loss: 0.4805\n",
      "Epoch 110/1000\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.2157 - output_1_loss: 0.2157 - val_loss: 0.5933 - val_output_1_loss: 0.5933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0d9534b668>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params after 1 hour of tuning\n",
    "tabnet = TabNet(num_features = train_X_transformed.shape[1],\n",
    "                output_dim = 128,\n",
    "                feature_dim = 128,\n",
    "                n_step = 2, \n",
    "                relaxation_factor= 2.2,\n",
    "                sparsity_coefficient=2.37e-07,\n",
    "                n_shared = 2,\n",
    "                bn_momentum = 0.9245)\n",
    "\n",
    "\n",
    "# Early stopping based on validation loss    \n",
    "cbs = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=30, restore_best_weights=True\n",
    "    )]\n",
    "\n",
    "# Optimiser \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=10)\n",
    "\n",
    "# Second loss in None because we also output the importances\n",
    "loss = [tf.keras.losses.CategoricalCrossentropy(from_logits=False), None]\n",
    "\n",
    "# Compile the model\n",
    "tabnet.compile(optimizer,\n",
    "               loss=loss)\n",
    "\n",
    "# Train the model\n",
    "tabnet.fit(train_ds, \n",
    "           epochs=1000, \n",
    "           validation_data=val_ds,\n",
    "           callbacks=cbs,\n",
    "           verbose=1,\n",
    "          class_weight={\n",
    "              0:1,\n",
    "              1: 10\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "val_preds, val_imps = tabnet.predict(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC 0.8505\n",
      "Test PR AUC 0.464\n"
     ]
    }
   ],
   "source": [
    "print('Test ROC AUC', np.round(roc_auc_score(val_y, val_preds[:, 1]), 4))\n",
    "print('Test PR AUC', np.round(average_precision_score(val_y, val_preds[:, 1]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>18403224</td>\n",
       "      <td>31.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10409</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>18403263</td>\n",
       "      <td>49.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4272</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>18403310</td>\n",
       "      <td>171.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4476</td>\n",
       "      <td>574.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>18403310</td>\n",
       "      <td>284.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10989</td>\n",
       "      <td>360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>18403317</td>\n",
       "      <td>67.95</td>\n",
       "      <td>W</td>\n",
       "      <td>18018</td>\n",
       "      <td>452.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  422 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
       "0        3663549       18403224           31.95         W  10409  111.0   \n",
       "1        3663550       18403263           49.00         W   4272  111.0   \n",
       "2        3663551       18403310          171.00         W   4476  574.0   \n",
       "3        3663552       18403310          284.95         W  10989  360.0   \n",
       "4        3663553       18403317           67.95         W  18018  452.0   \n",
       "\n",
       "   card3       card4  card5  card6  ...    id_32    id_33    id_34    id_35  \\\n",
       "0  150.0        visa  226.0  debit  ...  missing  missing  missing  missing   \n",
       "1  150.0        visa  226.0  debit  ...  missing  missing  missing  missing   \n",
       "2  150.0        visa  226.0  debit  ...  missing  missing  missing  missing   \n",
       "3  150.0        visa  166.0  debit  ...  missing  missing  missing  missing   \n",
       "4  150.0  mastercard  117.0  debit  ...  missing  missing  missing  missing   \n",
       "\n",
       "     id_36    id_37    id_38  DeviceType  DeviceInfo  hour  \n",
       "0  missing  missing  missing     missing     missing   0.0  \n",
       "1  missing  missing  missing     missing     missing   0.0  \n",
       "2  missing  missing  missing     missing     missing   0.0  \n",
       "3  missing  missing  missing     missing     missing   0.0  \n",
       "4  missing  missing  missing     missing     missing   0.0  \n",
       "\n",
       "[5 rows x 422 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds, test_imp = tabnet.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df  = pd.DataFrame({\"TransactionID\": test['TransactionID'].values,\n",
    "                              'isFraud': test_preds[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('tabnet_sumbission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
